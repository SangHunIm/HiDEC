{
    "path": {
        "train": "data/pregenerated/train.jsonl",
        "dev": "data/pregenerateddev.jsonl",
        "test": "data/pregeneratedtest.jsonl",
        "hierachy_node": "data/pregeneratedlabels.txt",
        "target_node": "data/pregeneratedtarget_labels.txt",
        "hierarchy_relation": "data/pregeneratedparent_child_map.txt",
        "vocab": "data/pregeneratedvocab.txt",
        "embedding": "data/glove.6B.300d.txt",
        "save": "./final_ckpt",
        "dataset": "EURLEX57K",
        "hparam_summary": "GRU-E1-D1-b128"
    },
    "batch_size": 128,
    "end_token_for_coarse_grained_labels":false,
    "trainer_hparams": {
        "max_epochs": 100,
        "accumulate_grad_batches": 1,
        "gradient_clip_val": 1.0,
        "accelerator": "gpu",
        "strategy": "ddp",
        "devices": [0,1],
        "num_sanity_val_steps": 0,
        "check_val_every_n_epoch": 2
    }, 
    "model_hparams": {
        "vocab_size": 60002,
        "text_embedding_size": 300,
        "label_embedding_size": 300,
        "encoder_type": "GRUOrig",
        "encoder_n_layers": 1,
        "encoder_hidden_size": 300,
        "encoder_bidirectional": true,
        "encoder_layer_norm": true,
        "decoder_n_layers": 1,
        "decoder_hidden_size": 300,
        "decoder_ffn_size": 600,
        "decoder_residual": false,
        "dropout": 0.1,
        "learning_rate": 0.0001,
        "weight_decay": 0.1,
        "warmup_proportion": 0.1,
        "adam_epsilon": 1e-08,
        "emb_scale": 1,
        "padding_idx": 0
    }
}